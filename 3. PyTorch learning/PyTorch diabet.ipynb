{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333])\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('diabetes.csv', delimiter =',', dtype=np.float32)\n",
    "\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0: -1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 10)\n",
    "        \n",
    "        self.l2 = torch.nn.R(10, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res_l1 = self.l1(x)\n",
    "        res_l2 = self.l2(res_l1)\n",
    "        res_l3 = self.l3(res_l2)\n",
    "        return self.sigmoid(res_l3)\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.8326832056045532 \n",
      "Epoch: 1 | Loss: 0.8119532465934753 \n",
      "Epoch: 2 | Loss: 0.7941680550575256 \n",
      "Epoch: 3 | Loss: 0.7787500619888306 \n",
      "Epoch: 4 | Loss: 0.7652618288993835 \n",
      "Epoch: 5 | Loss: 0.7533719539642334 \n",
      "Epoch: 6 | Loss: 0.7428204417228699 \n",
      "Epoch: 7 | Loss: 0.7334036827087402 \n",
      "Epoch: 8 | Loss: 0.7249572277069092 \n",
      "Epoch: 9 | Loss: 0.717348039150238 \n",
      "Epoch: 10 | Loss: 0.7104671597480774 \n",
      "Epoch: 11 | Loss: 0.7042251825332642 \n",
      "Epoch: 12 | Loss: 0.6985448002815247 \n",
      "Epoch: 13 | Loss: 0.6933644413948059 \n",
      "Epoch: 14 | Loss: 0.688628077507019 \n",
      "Epoch: 15 | Loss: 0.6842890381813049 \n",
      "Epoch: 16 | Loss: 0.6803072094917297 \n",
      "Epoch: 17 | Loss: 0.6766472458839417 \n",
      "Epoch: 18 | Loss: 0.6732791066169739 \n",
      "Epoch: 19 | Loss: 0.6701740026473999 \n",
      "Epoch: 20 | Loss: 0.6673093438148499 \n",
      "Epoch: 21 | Loss: 0.66466224193573 \n",
      "Epoch: 22 | Loss: 0.6622145771980286 \n",
      "Epoch: 23 | Loss: 0.6599488854408264 \n",
      "Epoch: 24 | Loss: 0.6578484177589417 \n",
      "Epoch: 25 | Loss: 0.6558999419212341 \n",
      "Epoch: 26 | Loss: 0.6540905833244324 \n",
      "Epoch: 27 | Loss: 0.6524083614349365 \n",
      "Epoch: 28 | Loss: 0.6508435010910034 \n",
      "Epoch: 29 | Loss: 0.649384617805481 \n",
      "Epoch: 30 | Loss: 0.6480241417884827 \n",
      "Epoch: 31 | Loss: 0.6467529535293579 \n",
      "Epoch: 32 | Loss: 0.6455639600753784 \n",
      "Epoch: 33 | Loss: 0.6444500088691711 \n",
      "Epoch: 34 | Loss: 0.6434047222137451 \n",
      "Epoch: 35 | Loss: 0.6424221992492676 \n",
      "Epoch: 36 | Loss: 0.6414969563484192 \n",
      "Epoch: 37 | Loss: 0.6406243443489075 \n",
      "Epoch: 38 | Loss: 0.6397992372512817 \n",
      "Epoch: 39 | Loss: 0.6390171647071838 \n",
      "Epoch: 40 | Loss: 0.6382739543914795 \n",
      "Epoch: 41 | Loss: 0.6375674605369568 \n",
      "Epoch: 42 | Loss: 0.6368920207023621 \n",
      "Epoch: 43 | Loss: 0.636247456073761 \n",
      "Epoch: 44 | Loss: 0.6356278657913208 \n",
      "Epoch: 45 | Loss: 0.6350324153900146 \n",
      "Epoch: 46 | Loss: 0.634458065032959 \n",
      "Epoch: 47 | Loss: 0.6339027881622314 \n",
      "Epoch: 48 | Loss: 0.6333639621734619 \n",
      "Epoch: 49 | Loss: 0.6328403353691101 \n",
      "Epoch: 50 | Loss: 0.6323291063308716 \n",
      "Epoch: 51 | Loss: 0.6318299174308777 \n",
      "Epoch: 52 | Loss: 0.6313404440879822 \n",
      "Epoch: 53 | Loss: 0.6308594942092896 \n",
      "Epoch: 54 | Loss: 0.6303861737251282 \n",
      "Epoch: 55 | Loss: 0.6299181580543518 \n",
      "Epoch: 56 | Loss: 0.6294553875923157 \n",
      "Epoch: 57 | Loss: 0.6289961338043213 \n",
      "Epoch: 58 | Loss: 0.6285399794578552 \n",
      "Epoch: 59 | Loss: 0.6280861496925354 \n",
      "Epoch: 60 | Loss: 0.6276334524154663 \n",
      "Epoch: 61 | Loss: 0.6271807551383972 \n",
      "Epoch: 62 | Loss: 0.6267280578613281 \n",
      "Epoch: 63 | Loss: 0.6262746453285217 \n",
      "Epoch: 64 | Loss: 0.6258193850517273 \n",
      "Epoch: 65 | Loss: 0.6253622174263 \n",
      "Epoch: 66 | Loss: 0.6249032020568848 \n",
      "Epoch: 67 | Loss: 0.6244397163391113 \n",
      "Epoch: 68 | Loss: 0.6239739656448364 \n",
      "Epoch: 69 | Loss: 0.6235036253929138 \n",
      "Epoch: 70 | Loss: 0.6230287551879883 \n",
      "Epoch: 71 | Loss: 0.6225495338439941 \n",
      "Epoch: 72 | Loss: 0.6220645904541016 \n",
      "Epoch: 73 | Loss: 0.6215748190879822 \n",
      "Epoch: 74 | Loss: 0.621078372001648 \n",
      "Epoch: 75 | Loss: 0.6205767393112183 \n",
      "Epoch: 76 | Loss: 0.6200677156448364 \n",
      "Epoch: 77 | Loss: 0.6195530891418457 \n",
      "Epoch: 78 | Loss: 0.6190311908721924 \n",
      "Epoch: 79 | Loss: 0.6185016632080078 \n",
      "Epoch: 80 | Loss: 0.6179649233818054 \n",
      "Epoch: 81 | Loss: 0.6174201369285583 \n",
      "Epoch: 82 | Loss: 0.6168672442436218 \n",
      "Epoch: 83 | Loss: 0.616306483745575 \n",
      "Epoch: 84 | Loss: 0.6157370805740356 \n",
      "Epoch: 85 | Loss: 0.6151595115661621 \n",
      "Epoch: 86 | Loss: 0.6145729422569275 \n",
      "Epoch: 87 | Loss: 0.6139769554138184 \n",
      "Epoch: 88 | Loss: 0.613371729850769 \n",
      "Epoch: 89 | Loss: 0.6127568483352661 \n",
      "Epoch: 90 | Loss: 0.6121326684951782 \n",
      "Epoch: 91 | Loss: 0.6114990711212158 \n",
      "Epoch: 92 | Loss: 0.6108543276786804 \n",
      "Epoch: 93 | Loss: 0.610200047492981 \n",
      "Epoch: 94 | Loss: 0.6095353364944458 \n",
      "Epoch: 95 | Loss: 0.6088595986366272 \n",
      "Epoch: 96 | Loss: 0.6081734895706177 \n",
      "Epoch: 97 | Loss: 0.607476532459259 \n",
      "Epoch: 98 | Loss: 0.6067684292793274 \n",
      "Epoch: 99 | Loss: 0.6060482859611511 \n",
      "Epoch: 100 | Loss: 0.6053175926208496 \n",
      "Epoch: 101 | Loss: 0.6045749187469482 \n",
      "Epoch: 102 | Loss: 0.6038212776184082 \n",
      "Epoch: 103 | Loss: 0.6030550599098206 \n",
      "Epoch: 104 | Loss: 0.6022772192955017 \n",
      "Epoch: 105 | Loss: 0.60148686170578 \n",
      "Epoch: 106 | Loss: 0.6006848812103271 \n",
      "Epoch: 107 | Loss: 0.5998697876930237 \n",
      "Epoch: 108 | Loss: 0.5990431904792786 \n",
      "Epoch: 109 | Loss: 0.5982027649879456 \n",
      "Epoch: 110 | Loss: 0.5973504185676575 \n",
      "Epoch: 111 | Loss: 0.5964856743812561 \n",
      "Epoch: 112 | Loss: 0.5956073999404907 \n",
      "Epoch: 113 | Loss: 0.5947167873382568 \n",
      "Epoch: 114 | Loss: 0.5938133001327515 \n",
      "Epoch: 115 | Loss: 0.5928967595100403 \n",
      "Epoch: 116 | Loss: 0.5919668078422546 \n",
      "Epoch: 117 | Loss: 0.5910241007804871 \n",
      "Epoch: 118 | Loss: 0.5900684595108032 \n",
      "Epoch: 119 | Loss: 0.5890997648239136 \n",
      "Epoch: 120 | Loss: 0.5881174206733704 \n",
      "Epoch: 121 | Loss: 0.5871231555938721 \n",
      "Epoch: 122 | Loss: 0.5861153602600098 \n",
      "Epoch: 123 | Loss: 0.585095226764679 \n",
      "Epoch: 124 | Loss: 0.5840618014335632 \n",
      "Epoch: 125 | Loss: 0.5830159187316895 \n",
      "Epoch: 126 | Loss: 0.5819576978683472 \n",
      "Epoch: 127 | Loss: 0.580886721611023 \n",
      "Epoch: 128 | Loss: 0.5798036456108093 \n",
      "Epoch: 129 | Loss: 0.5787085890769958 \n",
      "Epoch: 130 | Loss: 0.5776016712188721 \n",
      "Epoch: 131 | Loss: 0.576482892036438 \n",
      "Epoch: 132 | Loss: 0.5753529071807861 \n",
      "Epoch: 133 | Loss: 0.5742116570472717 \n",
      "Epoch: 134 | Loss: 0.5730602145195007 \n",
      "Epoch: 135 | Loss: 0.5718972682952881 \n",
      "Epoch: 136 | Loss: 0.5707249641418457 \n",
      "Epoch: 137 | Loss: 0.5695423483848572 \n",
      "Epoch: 138 | Loss: 0.5683503150939941 \n",
      "Epoch: 139 | Loss: 0.5671492218971252 \n",
      "Epoch: 140 | Loss: 0.5659396052360535 \n",
      "Epoch: 141 | Loss: 0.5647215247154236 \n",
      "Epoch: 142 | Loss: 0.563495934009552 \n",
      "Epoch: 143 | Loss: 0.5622633099555969 \n",
      "Epoch: 144 | Loss: 0.5610238909721375 \n",
      "Epoch: 145 | Loss: 0.559778094291687 \n",
      "Epoch: 146 | Loss: 0.5585271120071411 \n",
      "Epoch: 147 | Loss: 0.5572710633277893 \n",
      "Epoch: 148 | Loss: 0.5560104250907898 \n",
      "Epoch: 149 | Loss: 0.5547459125518799 \n",
      "Epoch: 150 | Loss: 0.5534782409667969 \n",
      "Epoch: 151 | Loss: 0.5522084832191467 \n",
      "Epoch: 152 | Loss: 0.5509366989135742 \n",
      "Epoch: 153 | Loss: 0.5496634840965271 \n",
      "Epoch: 154 | Loss: 0.5483901500701904 \n",
      "Epoch: 155 | Loss: 0.547116756439209 \n",
      "Epoch: 156 | Loss: 0.5458446145057678 \n",
      "Epoch: 157 | Loss: 0.5445739030838013 \n",
      "Epoch: 158 | Loss: 0.5433057546615601 \n",
      "Epoch: 159 | Loss: 0.542041003704071 \n",
      "Epoch: 160 | Loss: 0.5407795906066895 \n",
      "Epoch: 161 | Loss: 0.5395234227180481 \n",
      "Epoch: 162 | Loss: 0.5382722020149231 \n",
      "Epoch: 163 | Loss: 0.5370263457298279 \n",
      "Epoch: 164 | Loss: 0.5357878804206848 \n",
      "Epoch: 165 | Loss: 0.5345569252967834 \n",
      "Epoch: 166 | Loss: 0.5333337783813477 \n",
      "Epoch: 167 | Loss: 0.5321197509765625 \n",
      "Epoch: 168 | Loss: 0.5309149622917175 \n",
      "Epoch: 169 | Loss: 0.5297200083732605 \n",
      "Epoch: 170 | Loss: 0.5285360813140869 \n",
      "Epoch: 171 | Loss: 0.5273634791374207 \n",
      "Epoch: 172 | Loss: 0.5262027382850647 \n",
      "Epoch: 173 | Loss: 0.5250548720359802 \n",
      "Epoch: 174 | Loss: 0.5239191651344299 \n",
      "Epoch: 175 | Loss: 0.5227981209754944 \n",
      "Epoch: 176 | Loss: 0.5216898918151855 \n",
      "Epoch: 177 | Loss: 0.5205965638160706 \n",
      "Epoch: 178 | Loss: 0.5195179581642151 \n",
      "Epoch: 179 | Loss: 0.5184553861618042 \n",
      "Epoch: 180 | Loss: 0.5174075961112976 \n",
      "Epoch: 181 | Loss: 0.5163767337799072 \n",
      "Epoch: 182 | Loss: 0.5153613686561584 \n",
      "Epoch: 183 | Loss: 0.5143624544143677 \n",
      "Epoch: 184 | Loss: 0.5133803486824036 \n",
      "Epoch: 185 | Loss: 0.5124154090881348 \n",
      "Epoch: 186 | Loss: 0.5114679336547852 \n",
      "Epoch: 187 | Loss: 0.5105371475219727 \n",
      "Epoch: 188 | Loss: 0.5096243619918823 \n",
      "Epoch: 189 | Loss: 0.508728563785553 \n",
      "Epoch: 190 | Loss: 0.5078509449958801 \n",
      "Epoch: 191 | Loss: 0.5069905519485474 \n",
      "Epoch: 192 | Loss: 0.5061483979225159 \n",
      "Epoch: 193 | Loss: 0.5053233504295349 \n",
      "Epoch: 194 | Loss: 0.5045158863067627 \n",
      "Epoch: 195 | Loss: 0.503726065158844 \n",
      "Epoch: 196 | Loss: 0.5029537081718445 \n",
      "Epoch: 197 | Loss: 0.5021985769271851 \n",
      "Epoch: 198 | Loss: 0.5014610290527344 \n",
      "Epoch: 199 | Loss: 0.500740110874176 \n",
      "Epoch: 200 | Loss: 0.5000361204147339 \n",
      "Epoch: 201 | Loss: 0.49934953451156616 \n",
      "Epoch: 202 | Loss: 0.4986788034439087 \n",
      "Epoch: 203 | Loss: 0.49802422523498535 \n",
      "Epoch: 204 | Loss: 0.49738621711730957 \n",
      "Epoch: 205 | Loss: 0.49676376581192017 \n",
      "Epoch: 206 | Loss: 0.4961574673652649 \n",
      "Epoch: 207 | Loss: 0.49556607007980347 \n",
      "Epoch: 208 | Loss: 0.49498969316482544 \n",
      "Epoch: 209 | Loss: 0.4944286048412323 \n",
      "Epoch: 210 | Loss: 0.49388206005096436 \n",
      "Epoch: 211 | Loss: 0.4933495819568634 \n",
      "Epoch: 212 | Loss: 0.49283117055892944 \n",
      "Epoch: 213 | Loss: 0.49232640862464905 \n",
      "Epoch: 214 | Loss: 0.4918348789215088 \n",
      "Epoch: 215 | Loss: 0.49135711789131165 \n",
      "Epoch: 216 | Loss: 0.49089205265045166 \n",
      "Epoch: 217 | Loss: 0.4904397130012512 \n",
      "Epoch: 218 | Loss: 0.48999884724617004 \n",
      "Epoch: 219 | Loss: 0.4895705580711365 \n",
      "Epoch: 220 | Loss: 0.489154189825058 \n",
      "Epoch: 221 | Loss: 0.48874881863594055 \n",
      "Epoch: 222 | Loss: 0.48835474252700806 \n",
      "Epoch: 223 | Loss: 0.48797178268432617 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Loss: 0.487598717212677 \n",
      "Epoch: 225 | Loss: 0.4872360825538635 \n",
      "Epoch: 226 | Loss: 0.48688384890556335 \n",
      "Epoch: 227 | Loss: 0.48654142022132874 \n",
      "Epoch: 228 | Loss: 0.4862080216407776 \n",
      "Epoch: 229 | Loss: 0.4858843684196472 \n",
      "Epoch: 230 | Loss: 0.4855697751045227 \n",
      "Epoch: 231 | Loss: 0.48526275157928467 \n",
      "Epoch: 232 | Loss: 0.4849657416343689 \n",
      "Epoch: 233 | Loss: 0.4846761226654053 \n",
      "Epoch: 234 | Loss: 0.48439478874206543 \n",
      "Epoch: 235 | Loss: 0.4841214418411255 \n",
      "Epoch: 236 | Loss: 0.4838550090789795 \n",
      "Epoch: 237 | Loss: 0.48359614610671997 \n",
      "Epoch: 238 | Loss: 0.4833448529243469 \n",
      "Epoch: 239 | Loss: 0.4830998182296753 \n",
      "Epoch: 240 | Loss: 0.4828619062900543 \n",
      "Epoch: 241 | Loss: 0.48263075947761536 \n",
      "Epoch: 242 | Loss: 0.48240530490875244 \n",
      "Epoch: 243 | Loss: 0.48218613862991333 \n",
      "Epoch: 244 | Loss: 0.4819727838039398 \n",
      "Epoch: 245 | Loss: 0.4817659556865692 \n",
      "Epoch: 246 | Loss: 0.48156437277793884 \n",
      "Epoch: 247 | Loss: 0.48136794567108154 \n",
      "Epoch: 248 | Loss: 0.4811770021915436 \n",
      "Epoch: 249 | Loss: 0.48099076747894287 \n",
      "Epoch: 250 | Loss: 0.4808100461959839 \n",
      "Epoch: 251 | Loss: 0.48063385486602783 \n",
      "Epoch: 252 | Loss: 0.4804627299308777 \n",
      "Epoch: 253 | Loss: 0.4802956283092499 \n",
      "Epoch: 254 | Loss: 0.48013296723365784 \n",
      "Epoch: 255 | Loss: 0.4799751937389374 \n",
      "Epoch: 256 | Loss: 0.4798208177089691 \n",
      "Epoch: 257 | Loss: 0.47967100143432617 \n",
      "Epoch: 258 | Loss: 0.4795244336128235 \n",
      "Epoch: 259 | Loss: 0.4793822169303894 \n",
      "Epoch: 260 | Loss: 0.4792431890964508 \n",
      "Epoch: 261 | Loss: 0.47910788655281067 \n",
      "Epoch: 262 | Loss: 0.47897645831108093 \n",
      "Epoch: 263 | Loss: 0.47884783148765564 \n",
      "Epoch: 264 | Loss: 0.47872281074523926 \n",
      "Epoch: 265 | Loss: 0.4786005914211273 \n",
      "Epoch: 266 | Loss: 0.4784812033176422 \n",
      "Epoch: 267 | Loss: 0.478365421295166 \n",
      "Epoch: 268 | Loss: 0.4782521426677704 \n",
      "Epoch: 269 | Loss: 0.4781416058540344 \n",
      "Epoch: 270 | Loss: 0.4780339002609253 \n",
      "Epoch: 271 | Loss: 0.4779285192489624 \n",
      "Epoch: 272 | Loss: 0.47782641649246216 \n",
      "Epoch: 273 | Loss: 0.4777258634567261 \n",
      "Epoch: 274 | Loss: 0.47762829065322876 \n",
      "Epoch: 275 | Loss: 0.4775327444076538 \n",
      "Epoch: 276 | Loss: 0.47744014859199524 \n",
      "Epoch: 277 | Loss: 0.4773485064506531 \n",
      "Epoch: 278 | Loss: 0.47725990414619446 \n",
      "Epoch: 279 | Loss: 0.47717317938804626 \n",
      "Epoch: 280 | Loss: 0.4770887792110443 \n",
      "Epoch: 281 | Loss: 0.47700566053390503 \n",
      "Epoch: 282 | Loss: 0.4769245386123657 \n",
      "Epoch: 283 | Loss: 0.4768458902835846 \n",
      "Epoch: 284 | Loss: 0.47676894068717957 \n",
      "Epoch: 285 | Loss: 0.47669312357902527 \n",
      "Epoch: 286 | Loss: 0.47661954164505005 \n",
      "Epoch: 287 | Loss: 0.47654709219932556 \n",
      "Epoch: 288 | Loss: 0.47647640109062195 \n",
      "Epoch: 289 | Loss: 0.4764070212841034 \n",
      "Epoch: 290 | Loss: 0.47633981704711914 \n",
      "Epoch: 291 | Loss: 0.4762739837169647 \n",
      "Epoch: 292 | Loss: 0.4762097895145416 \n",
      "Epoch: 293 | Loss: 0.4761464297771454 \n",
      "Epoch: 294 | Loss: 0.47608423233032227 \n",
      "Epoch: 295 | Loss: 0.4760235846042633 \n",
      "Epoch: 296 | Loss: 0.4759644567966461 \n",
      "Epoch: 297 | Loss: 0.47590646147727966 \n",
      "Epoch: 298 | Loss: 0.4758492112159729 \n",
      "Epoch: 299 | Loss: 0.4757938086986542 \n",
      "Epoch: 300 | Loss: 0.47573891282081604 \n",
      "Epoch: 301 | Loss: 0.4756852388381958 \n",
      "Epoch: 302 | Loss: 0.4756333827972412 \n",
      "Epoch: 303 | Loss: 0.4755820631980896 \n",
      "Epoch: 304 | Loss: 0.4755314886569977 \n",
      "Epoch: 305 | Loss: 0.47548219561576843 \n",
      "Epoch: 306 | Loss: 0.4754340350627899 \n",
      "Epoch: 307 | Loss: 0.4753868281841278 \n",
      "Epoch: 308 | Loss: 0.4753400683403015 \n",
      "Epoch: 309 | Loss: 0.47529470920562744 \n",
      "Epoch: 310 | Loss: 0.47524964809417725 \n",
      "Epoch: 311 | Loss: 0.4752058684825897 \n",
      "Epoch: 312 | Loss: 0.47516292333602905 \n",
      "Epoch: 313 | Loss: 0.47512054443359375 \n",
      "Epoch: 314 | Loss: 0.47507908940315247 \n",
      "Epoch: 315 | Loss: 0.47503870725631714 \n",
      "Epoch: 316 | Loss: 0.47499826550483704 \n",
      "Epoch: 317 | Loss: 0.4749593138694763 \n",
      "Epoch: 318 | Loss: 0.47492074966430664 \n",
      "Epoch: 319 | Loss: 0.47488296031951904 \n",
      "Epoch: 320 | Loss: 0.47484612464904785 \n",
      "Epoch: 321 | Loss: 0.4748094379901886 \n",
      "Epoch: 322 | Loss: 0.47477346658706665 \n",
      "Epoch: 323 | Loss: 0.47473835945129395 \n",
      "Epoch: 324 | Loss: 0.47470372915267944 \n",
      "Epoch: 325 | Loss: 0.47466936707496643 \n",
      "Epoch: 326 | Loss: 0.4746360182762146 \n",
      "Epoch: 327 | Loss: 0.47460341453552246 \n",
      "Epoch: 328 | Loss: 0.4745708703994751 \n",
      "Epoch: 329 | Loss: 0.47453922033309937 \n",
      "Epoch: 330 | Loss: 0.47450780868530273 \n",
      "Epoch: 331 | Loss: 0.47447702288627625 \n",
      "Epoch: 332 | Loss: 0.4744468927383423 \n",
      "Epoch: 333 | Loss: 0.4744164049625397 \n",
      "Epoch: 334 | Loss: 0.4743874967098236 \n",
      "Epoch: 335 | Loss: 0.47435855865478516 \n",
      "Epoch: 336 | Loss: 0.4743301272392273 \n",
      "Epoch: 337 | Loss: 0.47430258989334106 \n",
      "Epoch: 338 | Loss: 0.47427457571029663 \n",
      "Epoch: 339 | Loss: 0.4742475748062134 \n",
      "Epoch: 340 | Loss: 0.47422119975090027 \n",
      "Epoch: 341 | Loss: 0.47419506311416626 \n",
      "Epoch: 342 | Loss: 0.4741688668727875 \n",
      "Epoch: 343 | Loss: 0.4741436541080475 \n",
      "Epoch: 344 | Loss: 0.47411808371543884 \n",
      "Epoch: 345 | Loss: 0.4740939140319824 \n",
      "Epoch: 346 | Loss: 0.4740692973136902 \n",
      "Epoch: 347 | Loss: 0.4740449786186218 \n",
      "Epoch: 348 | Loss: 0.4740212857723236 \n",
      "Epoch: 349 | Loss: 0.47399795055389404 \n",
      "Epoch: 350 | Loss: 0.47397497296333313 \n",
      "Epoch: 351 | Loss: 0.47395238280296326 \n",
      "Epoch: 352 | Loss: 0.4739300310611725 \n",
      "Epoch: 353 | Loss: 0.473908007144928 \n",
      "Epoch: 354 | Loss: 0.47388583421707153 \n",
      "Epoch: 355 | Loss: 0.4738645553588867 \n",
      "Epoch: 356 | Loss: 0.47384342551231384 \n",
      "Epoch: 357 | Loss: 0.47382283210754395 \n",
      "Epoch: 358 | Loss: 0.4738016724586487 \n",
      "Epoch: 359 | Loss: 0.4737814962863922 \n",
      "Epoch: 360 | Loss: 0.47376158833503723 \n",
      "Epoch: 361 | Loss: 0.47374188899993896 \n",
      "Epoch: 362 | Loss: 0.47372204065322876 \n",
      "Epoch: 363 | Loss: 0.4737030565738678 \n",
      "Epoch: 364 | Loss: 0.4736841320991516 \n",
      "Epoch: 365 | Loss: 0.4736652970314026 \n",
      "Epoch: 366 | Loss: 0.4736466109752655 \n",
      "Epoch: 367 | Loss: 0.47362789511680603 \n",
      "Epoch: 368 | Loss: 0.4736103415489197 \n",
      "Epoch: 369 | Loss: 0.4735921621322632 \n",
      "Epoch: 370 | Loss: 0.4735746383666992 \n",
      "Epoch: 371 | Loss: 0.4735572338104248 \n",
      "Epoch: 372 | Loss: 0.4735400080680847 \n",
      "Epoch: 373 | Loss: 0.4735225737094879 \n",
      "Epoch: 374 | Loss: 0.47350603342056274 \n",
      "Epoch: 375 | Loss: 0.47348958253860474 \n",
      "Epoch: 376 | Loss: 0.4734733998775482 \n",
      "Epoch: 377 | Loss: 0.47345685958862305 \n",
      "Epoch: 378 | Loss: 0.47344040870666504 \n",
      "Epoch: 379 | Loss: 0.4734244644641876 \n",
      "Epoch: 380 | Loss: 0.4734092652797699 \n",
      "Epoch: 381 | Loss: 0.473393976688385 \n",
      "Epoch: 382 | Loss: 0.47337862849235535 \n",
      "Epoch: 383 | Loss: 0.47336310148239136 \n",
      "Epoch: 384 | Loss: 0.4733482897281647 \n",
      "Epoch: 385 | Loss: 0.4733333885669708 \n",
      "Epoch: 386 | Loss: 0.4733191430568695 \n",
      "Epoch: 387 | Loss: 0.4733041822910309 \n",
      "Epoch: 388 | Loss: 0.47329023480415344 \n",
      "Epoch: 389 | Loss: 0.4732758402824402 \n",
      "Epoch: 390 | Loss: 0.47326162457466125 \n",
      "Epoch: 391 | Loss: 0.4732479453086853 \n",
      "Epoch: 392 | Loss: 0.4732344448566437 \n",
      "Epoch: 393 | Loss: 0.47322043776512146 \n",
      "Epoch: 394 | Loss: 0.4732077121734619 \n",
      "Epoch: 395 | Loss: 0.47319427132606506 \n",
      "Epoch: 396 | Loss: 0.47318118810653687 \n",
      "Epoch: 397 | Loss: 0.4731682538986206 \n",
      "Epoch: 398 | Loss: 0.47315558791160583 \n",
      "Epoch: 399 | Loss: 0.4731423854827881 \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(400):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for test: \n",
      "0.9231531023979187\n"
     ]
    }
   ],
   "source": [
    "#prediction after training - expect 1\n",
    "var1 = torch.tensor([-0.882353,-0.0653266,0.147541,-0.373737,0,-0.0938897,-0.797609,-0.933333])\n",
    "y_pred = model(var1)\n",
    "print(\"result for test: \")\n",
    "print(y_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
