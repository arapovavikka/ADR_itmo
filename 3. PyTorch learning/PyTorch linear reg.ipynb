{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "#https://www.youtube.com/watch?v=113b7O3mabY&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.01568392477929592 \n",
      "Epoch: 1 | Loss: 0.01545854564756155 \n",
      "Epoch: 2 | Loss: 0.015236406587064266 \n",
      "Epoch: 3 | Loss: 0.015017449855804443 \n",
      "Epoch: 4 | Loss: 0.014801583252847195 \n",
      "Epoch: 5 | Loss: 0.014588897116482258 \n",
      "Epoch: 6 | Loss: 0.014379218220710754 \n",
      "Epoch: 7 | Loss: 0.014172540977597237 \n",
      "Epoch: 8 | Loss: 0.013968849554657936 \n",
      "Epoch: 9 | Loss: 0.01376811321824789 \n",
      "Epoch: 10 | Loss: 0.013570251874625683 \n",
      "Epoch: 11 | Loss: 0.013375242240726948 \n",
      "Epoch: 12 | Loss: 0.01318296603858471 \n",
      "Epoch: 13 | Loss: 0.012993532232940197 \n",
      "Epoch: 14 | Loss: 0.012806791812181473 \n",
      "Epoch: 15 | Loss: 0.012622782960534096 \n",
      "Epoch: 16 | Loss: 0.012441311031579971 \n",
      "Epoch: 17 | Loss: 0.01226254366338253 \n",
      "Epoch: 18 | Loss: 0.012086311355233192 \n",
      "Epoch: 19 | Loss: 0.011912628076970577 \n",
      "Epoch: 20 | Loss: 0.0117413941770792 \n",
      "Epoch: 21 | Loss: 0.011572643183171749 \n",
      "Epoch: 22 | Loss: 0.011406331323087215 \n",
      "Epoch: 23 | Loss: 0.01124242041260004 \n",
      "Epoch: 24 | Loss: 0.011080838739871979 \n",
      "Epoch: 25 | Loss: 0.010921605862677097 \n",
      "Epoch: 26 | Loss: 0.010764660313725471 \n",
      "Epoch: 27 | Loss: 0.010609909892082214 \n",
      "Epoch: 28 | Loss: 0.010457445867359638 \n",
      "Epoch: 29 | Loss: 0.010307135060429573 \n",
      "Epoch: 30 | Loss: 0.010159015655517578 \n",
      "Epoch: 31 | Loss: 0.010013015009462833 \n",
      "Epoch: 32 | Loss: 0.009869149886071682 \n",
      "Epoch: 33 | Loss: 0.0097272964194417 \n",
      "Epoch: 34 | Loss: 0.009587460197508335 \n",
      "Epoch: 35 | Loss: 0.009449698030948639 \n",
      "Epoch: 36 | Loss: 0.009313883259892464 \n",
      "Epoch: 37 | Loss: 0.009180063381791115 \n",
      "Epoch: 38 | Loss: 0.009048102423548698 \n",
      "Epoch: 39 | Loss: 0.008918064646422863 \n",
      "Epoch: 40 | Loss: 0.00878988765180111 \n",
      "Epoch: 41 | Loss: 0.008663571439683437 \n",
      "Epoch: 42 | Loss: 0.008539054542779922 \n",
      "Epoch: 43 | Loss: 0.008416330441832542 \n",
      "Epoch: 44 | Loss: 0.008295377716422081 \n",
      "Epoch: 45 | Loss: 0.008176185190677643 \n",
      "Epoch: 46 | Loss: 0.008058663457632065 \n",
      "Epoch: 47 | Loss: 0.007942816242575645 \n",
      "Epoch: 48 | Loss: 0.007828672416508198 \n",
      "Epoch: 49 | Loss: 0.007716212887316942 \n",
      "Epoch: 50 | Loss: 0.007605281658470631 \n",
      "Epoch: 51 | Loss: 0.007496014703065157 \n",
      "Epoch: 52 | Loss: 0.007388288155198097 \n",
      "Epoch: 53 | Loss: 0.007282084785401821 \n",
      "Epoch: 54 | Loss: 0.007177429273724556 \n",
      "Epoch: 55 | Loss: 0.00707428902387619 \n",
      "Epoch: 56 | Loss: 0.0069725848734378815 \n",
      "Epoch: 57 | Loss: 0.006872368976473808 \n",
      "Epoch: 58 | Loss: 0.0067736441269516945 \n",
      "Epoch: 59 | Loss: 0.006676303222775459 \n",
      "Epoch: 60 | Loss: 0.0065803369507193565 \n",
      "Epoch: 61 | Loss: 0.006485781632363796 \n",
      "Epoch: 62 | Loss: 0.006392552051693201 \n",
      "Epoch: 63 | Loss: 0.006300731562077999 \n",
      "Epoch: 64 | Loss: 0.006210106424987316 \n",
      "Epoch: 65 | Loss: 0.006120883859694004 \n",
      "Epoch: 66 | Loss: 0.006032930221408606 \n",
      "Epoch: 67 | Loss: 0.005946232005953789 \n",
      "Epoch: 68 | Loss: 0.005860744044184685 \n",
      "Epoch: 69 | Loss: 0.0057765161618590355 \n",
      "Epoch: 70 | Loss: 0.005693529732525349 \n",
      "Epoch: 71 | Loss: 0.005611666012555361 \n",
      "Epoch: 72 | Loss: 0.0055310484021902084 \n",
      "Epoch: 73 | Loss: 0.005451555363833904 \n",
      "Epoch: 74 | Loss: 0.005373203195631504 \n",
      "Epoch: 75 | Loss: 0.005296024959534407 \n",
      "Epoch: 76 | Loss: 0.005219882819801569 \n",
      "Epoch: 77 | Loss: 0.005144834518432617 \n",
      "Epoch: 78 | Loss: 0.005070938263088465 \n",
      "Epoch: 79 | Loss: 0.004998028744012117 \n",
      "Epoch: 80 | Loss: 0.004926193971186876 \n",
      "Epoch: 81 | Loss: 0.004855417646467686 \n",
      "Epoch: 82 | Loss: 0.004785644821822643 \n",
      "Epoch: 83 | Loss: 0.004716875962913036 \n",
      "Epoch: 84 | Loss: 0.004649088717997074 \n",
      "Epoch: 85 | Loss: 0.004582256078720093 \n",
      "Epoch: 86 | Loss: 0.004516431130468845 \n",
      "Epoch: 87 | Loss: 0.0044515011832118034 \n",
      "Epoch: 88 | Loss: 0.004387511406093836 \n",
      "Epoch: 89 | Loss: 0.004324485547840595 \n",
      "Epoch: 90 | Loss: 0.0042623067274689674 \n",
      "Epoch: 91 | Loss: 0.004201030824333429 \n",
      "Epoch: 92 | Loss: 0.004140699282288551 \n",
      "Epoch: 93 | Loss: 0.004081164952367544 \n",
      "Epoch: 94 | Loss: 0.0040225074626505375 \n",
      "Epoch: 95 | Loss: 0.003964732401072979 \n",
      "Epoch: 96 | Loss: 0.003907728008925915 \n",
      "Epoch: 97 | Loss: 0.0038515729829669 \n",
      "Epoch: 98 | Loss: 0.003796250792220235 \n",
      "Epoch: 99 | Loss: 0.003741652239114046 \n",
      "Epoch: 100 | Loss: 0.0036878902465105057 \n",
      "Epoch: 101 | Loss: 0.003634864930063486 \n",
      "Epoch: 102 | Loss: 0.0035826372914016247 \n",
      "Epoch: 103 | Loss: 0.0035311472602188587 \n",
      "Epoch: 104 | Loss: 0.0034804106689989567 \n",
      "Epoch: 105 | Loss: 0.0034303816501051188 \n",
      "Epoch: 106 | Loss: 0.0033811067696660757 \n",
      "Epoch: 107 | Loss: 0.003332497552037239 \n",
      "Epoch: 108 | Loss: 0.0032845865935087204 \n",
      "Epoch: 109 | Loss: 0.0032373764552176 \n",
      "Epoch: 110 | Loss: 0.0031908925157040358 \n",
      "Epoch: 111 | Loss: 0.003144992748275399 \n",
      "Epoch: 112 | Loss: 0.0030998040456324816 \n",
      "Epoch: 113 | Loss: 0.0030552796088159084 \n",
      "Epoch: 114 | Loss: 0.003011345397680998 \n",
      "Epoch: 115 | Loss: 0.002968084067106247 \n",
      "Epoch: 116 | Loss: 0.0029254157561808825 \n",
      "Epoch: 117 | Loss: 0.002883384469896555 \n",
      "Epoch: 118 | Loss: 0.0028419173322618008 \n",
      "Epoch: 119 | Loss: 0.002801089081913233 \n",
      "Epoch: 120 | Loss: 0.002760849893093109 \n",
      "Epoch: 121 | Loss: 0.002721183467656374 \n",
      "Epoch: 122 | Loss: 0.002682063262909651 \n",
      "Epoch: 123 | Loss: 0.0026435041800141335 \n",
      "Epoch: 124 | Loss: 0.0026055241469293833 \n",
      "Epoch: 125 | Loss: 0.00256807217374444 \n",
      "Epoch: 126 | Loss: 0.0025311722420156 \n",
      "Epoch: 127 | Loss: 0.002494771732017398 \n",
      "Epoch: 128 | Loss: 0.002458923263475299 \n",
      "Epoch: 129 | Loss: 0.0024236058816313744 \n",
      "Epoch: 130 | Loss: 0.002388759981840849 \n",
      "Epoch: 131 | Loss: 0.0023544346913695335 \n",
      "Epoch: 132 | Loss: 0.0023206183686852455 \n",
      "Epoch: 133 | Loss: 0.002287243725731969 \n",
      "Epoch: 134 | Loss: 0.002254362218081951 \n",
      "Epoch: 135 | Loss: 0.0022219927050173283 \n",
      "Epoch: 136 | Loss: 0.002190037164837122 \n",
      "Epoch: 137 | Loss: 0.0021585505455732346 \n",
      "Epoch: 138 | Loss: 0.0021275521721690893 \n",
      "Epoch: 139 | Loss: 0.002096980344504118 \n",
      "Epoch: 140 | Loss: 0.002066830638796091 \n",
      "Epoch: 141 | Loss: 0.0020371433347463608 \n",
      "Epoch: 142 | Loss: 0.002007853239774704 \n",
      "Epoch: 143 | Loss: 0.0019789873622357845 \n",
      "Epoch: 144 | Loss: 0.0019505434902384877 \n",
      "Epoch: 145 | Loss: 0.00192253477871418 \n",
      "Epoch: 146 | Loss: 0.001894893473945558 \n",
      "Epoch: 147 | Loss: 0.0018676456529647112 \n",
      "Epoch: 148 | Loss: 0.0018408347386866808 \n",
      "Epoch: 149 | Loss: 0.001814363757148385 \n",
      "Epoch: 150 | Loss: 0.0017882885877043009 \n",
      "Epoch: 151 | Loss: 0.0017626045737415552 \n",
      "Epoch: 152 | Loss: 0.0017372446600347757 \n",
      "Epoch: 153 | Loss: 0.0017122923163697124 \n",
      "Epoch: 154 | Loss: 0.0016876922454684973 \n",
      "Epoch: 155 | Loss: 0.001663438742980361 \n",
      "Epoch: 156 | Loss: 0.0016395066631957889 \n",
      "Epoch: 157 | Loss: 0.0016159676015377045 \n",
      "Epoch: 158 | Loss: 0.0015927357599139214 \n",
      "Epoch: 159 | Loss: 0.0015698380302637815 \n",
      "Epoch: 160 | Loss: 0.0015472820959985256 \n",
      "Epoch: 161 | Loss: 0.0015250574797391891 \n",
      "Epoch: 162 | Loss: 0.0015031311195343733 \n",
      "Epoch: 163 | Loss: 0.0014815314207226038 \n",
      "Epoch: 164 | Loss: 0.0014602325391024351 \n",
      "Epoch: 165 | Loss: 0.0014392553130164742 \n",
      "Epoch: 166 | Loss: 0.0014185787877067924 \n",
      "Epoch: 167 | Loss: 0.0013981759548187256 \n",
      "Epoch: 168 | Loss: 0.001378089189529419 \n",
      "Epoch: 169 | Loss: 0.0013582720421254635 \n",
      "Epoch: 170 | Loss: 0.0013387601356953382 \n",
      "Epoch: 171 | Loss: 0.0013195096980780363 \n",
      "Epoch: 172 | Loss: 0.0013005643850192428 \n",
      "Epoch: 173 | Loss: 0.0012818747200071812 \n",
      "Epoch: 174 | Loss: 0.0012634443119168282 \n",
      "Epoch: 175 | Loss: 0.0012452889932319522 \n",
      "Epoch: 176 | Loss: 0.0012273932807147503 \n",
      "Epoch: 177 | Loss: 0.0012097384314984083 \n",
      "Epoch: 178 | Loss: 0.0011923640267923474 \n",
      "Epoch: 179 | Loss: 0.0011752150021493435 \n",
      "Epoch: 180 | Loss: 0.0011583305895328522 \n",
      "Epoch: 181 | Loss: 0.001141692278906703 \n",
      "Epoch: 182 | Loss: 0.0011252774856984615 \n",
      "Epoch: 183 | Loss: 0.0011091036722064018 \n",
      "Epoch: 184 | Loss: 0.0010931680444628 \n",
      "Epoch: 185 | Loss: 0.0010774641996249557 \n",
      "Epoch: 186 | Loss: 0.0010619648965075612 \n",
      "Epoch: 187 | Loss: 0.0010467246174812317 \n",
      "Epoch: 188 | Loss: 0.0010316690895706415 \n",
      "Epoch: 189 | Loss: 0.0010168469743803144 \n",
      "Epoch: 190 | Loss: 0.001002224045805633 \n",
      "Epoch: 191 | Loss: 0.0009878368582576513 \n",
      "Epoch: 192 | Loss: 0.0009736262145452201 \n",
      "Epoch: 193 | Loss: 0.0009596338495612144 \n",
      "Epoch: 194 | Loss: 0.0009458321728743613 \n",
      "Epoch: 195 | Loss: 0.0009322529076598585 \n",
      "Epoch: 196 | Loss: 0.0009188475669361651 \n",
      "Epoch: 197 | Loss: 0.0009056496783159673 \n",
      "Epoch: 198 | Loss: 0.0008926443988457322 \n",
      "Epoch: 199 | Loss: 0.0008797987829893827 \n",
      "Epoch: 200 | Loss: 0.0008671507239341736 \n",
      "Epoch: 201 | Loss: 0.0008546945755369961 \n",
      "Epoch: 202 | Loss: 0.000842404377181083 \n",
      "Epoch: 203 | Loss: 0.0008302936912514269 \n",
      "Epoch: 204 | Loss: 0.0008183805039152503 \n",
      "Epoch: 205 | Loss: 0.0008066028822213411 \n",
      "Epoch: 206 | Loss: 0.0007950189174152911 \n",
      "Epoch: 207 | Loss: 0.0007835936266928911 \n",
      "Epoch: 208 | Loss: 0.0007723253220319748 \n",
      "Epoch: 209 | Loss: 0.0007612283807247877 \n",
      "Epoch: 210 | Loss: 0.0007502882508561015 \n",
      "Epoch: 211 | Loss: 0.0007395095308311284 \n",
      "Epoch: 212 | Loss: 0.0007288872729986906 \n",
      "Epoch: 213 | Loss: 0.0007184026180766523 \n",
      "Epoch: 214 | Loss: 0.0007080914219841361 \n",
      "Epoch: 215 | Loss: 0.0006979070021770895 \n",
      "Epoch: 216 | Loss: 0.000687877181917429 \n",
      "Epoch: 217 | Loss: 0.0006779971299692988 \n",
      "Epoch: 218 | Loss: 0.0006682471139356494 \n",
      "Epoch: 219 | Loss: 0.0006586425006389618 \n",
      "Epoch: 220 | Loss: 0.0006491768872365355 \n",
      "Epoch: 221 | Loss: 0.0006398357218131423 \n",
      "Epoch: 222 | Loss: 0.0006306556751951575 \n",
      "Epoch: 223 | Loss: 0.0006215912871994078 \n",
      "Epoch: 224 | Loss: 0.0006126428488641977 \n",
      "Epoch: 225 | Loss: 0.0006038550636731088 \n",
      "Epoch: 226 | Loss: 0.0005951747298240662 \n",
      "Epoch: 227 | Loss: 0.0005866205901838839 \n",
      "Epoch: 228 | Loss: 0.0005781870568171144 \n",
      "Epoch: 229 | Loss: 0.0005698895547538996 \n",
      "Epoch: 230 | Loss: 0.0005616837879642844 \n",
      "Epoch: 231 | Loss: 0.0005536099779419601 \n",
      "Epoch: 232 | Loss: 0.0005456666112877429 \n",
      "Epoch: 233 | Loss: 0.0005378170753829181 \n",
      "Epoch: 234 | Loss: 0.0005300872726365924 \n",
      "Epoch: 235 | Loss: 0.0005224613123573363 \n",
      "Epoch: 236 | Loss: 0.0005149618373252451 \n",
      "Epoch: 237 | Loss: 0.0005075506051070988 \n",
      "Epoch: 238 | Loss: 0.0005002659745514393 \n",
      "Epoch: 239 | Loss: 0.0004930817522108555 \n",
      "Epoch: 240 | Loss: 0.0004859853652305901 \n",
      "Epoch: 241 | Loss: 0.0004789925878867507 \n",
      "Epoch: 242 | Loss: 0.0004721124132629484 \n",
      "Epoch: 243 | Loss: 0.00046534097054973245 \n",
      "Epoch: 244 | Loss: 0.0004586532595567405 \n",
      "Epoch: 245 | Loss: 0.0004520622023846954 \n",
      "Epoch: 246 | Loss: 0.00044555074418894947 \n",
      "Epoch: 247 | Loss: 0.00043915596324950457 \n",
      "Epoch: 248 | Loss: 0.0004328509676270187 \n",
      "Epoch: 249 | Loss: 0.0004266252217348665 \n",
      "Epoch: 250 | Loss: 0.00042049220064654946 \n",
      "Epoch: 251 | Loss: 0.0004144450940657407 \n",
      "Epoch: 252 | Loss: 0.00040849813376553357 \n",
      "Epoch: 253 | Loss: 0.0004026268143206835 \n",
      "Epoch: 254 | Loss: 0.0003968304372392595 \n",
      "Epoch: 255 | Loss: 0.0003911301610060036 \n",
      "Epoch: 256 | Loss: 0.00038551329635083675 \n",
      "Epoch: 257 | Loss: 0.00037997227627784014 \n",
      "Epoch: 258 | Loss: 0.0003745118447113782 \n",
      "Epoch: 259 | Loss: 0.0003691322635859251 \n",
      "Epoch: 260 | Loss: 0.00036382704274728894 \n",
      "Epoch: 261 | Loss: 0.00035859004128724337 \n",
      "Epoch: 262 | Loss: 0.0003534434363245964 \n",
      "Epoch: 263 | Loss: 0.0003483643813524395 \n",
      "Epoch: 264 | Loss: 0.0003433544188737869 \n",
      "Epoch: 265 | Loss: 0.0003384214360266924 \n",
      "Epoch: 266 | Loss: 0.00033356339554302394 \n",
      "Epoch: 267 | Loss: 0.00032876263139769435 \n",
      "Epoch: 268 | Loss: 0.0003240374499000609 \n",
      "Epoch: 269 | Loss: 0.00031937664607539773 \n",
      "Epoch: 270 | Loss: 0.0003147877869196236 \n",
      "Epoch: 271 | Loss: 0.000310263829305768 \n",
      "Epoch: 272 | Loss: 0.0003058041911572218 \n",
      "Epoch: 273 | Loss: 0.0003014121321029961 \n",
      "Epoch: 274 | Loss: 0.0002970839268527925 \n",
      "Epoch: 275 | Loss: 0.0002928069152403623 \n",
      "Epoch: 276 | Loss: 0.0002886004513129592 \n",
      "Epoch: 277 | Loss: 0.0002844498958438635 \n",
      "Epoch: 278 | Loss: 0.0002803702955134213 \n",
      "Epoch: 279 | Loss: 0.00027633848367258906 \n",
      "Epoch: 280 | Loss: 0.00027236566529609263 \n",
      "Epoch: 281 | Loss: 0.0002684521023184061 \n",
      "Epoch: 282 | Loss: 0.0002645942731760442 \n",
      "Epoch: 283 | Loss: 0.0002607841161079705 \n",
      "Epoch: 284 | Loss: 0.0002570435171946883 \n",
      "Epoch: 285 | Loss: 0.0002533493679948151 \n",
      "Epoch: 286 | Loss: 0.0002497039968147874 \n",
      "Epoch: 287 | Loss: 0.0002461124095134437 \n",
      "Epoch: 288 | Loss: 0.00024258218763861805 \n",
      "Epoch: 289 | Loss: 0.00023910081654321402 \n",
      "Epoch: 290 | Loss: 0.00023565445735584944 \n",
      "Epoch: 291 | Loss: 0.00023227118072099984 \n",
      "Epoch: 292 | Loss: 0.00022892648121342063 \n",
      "Epoch: 293 | Loss: 0.00022564442770089954 \n",
      "Epoch: 294 | Loss: 0.00022239823010750115 \n",
      "Epoch: 295 | Loss: 0.0002192039682995528 \n",
      "Epoch: 296 | Loss: 0.00021605673828162253 \n",
      "Epoch: 297 | Loss: 0.00021295007900334895 \n",
      "Epoch: 298 | Loss: 0.0002098937111441046 \n",
      "Epoch: 299 | Loss: 0.00020687701180577278 \n",
      "Epoch: 300 | Loss: 0.0002039045502897352 \n",
      "Epoch: 301 | Loss: 0.00020096846856176853 \n",
      "Epoch: 302 | Loss: 0.00019807901117019355 \n",
      "Epoch: 303 | Loss: 0.00019523325318004936 \n",
      "Epoch: 304 | Loss: 0.00019242579583078623 \n",
      "Epoch: 305 | Loss: 0.0001896643079817295 \n",
      "Epoch: 306 | Loss: 0.0001869331463240087 \n",
      "Epoch: 307 | Loss: 0.00018425253801979125 \n",
      "Epoch: 308 | Loss: 0.0001816069707274437 \n",
      "Epoch: 309 | Loss: 0.0001789875386748463 \n",
      "Epoch: 310 | Loss: 0.0001764226472005248 \n",
      "Epoch: 311 | Loss: 0.00017388009291607887 \n",
      "Epoch: 312 | Loss: 0.00017137885151896626 \n",
      "Epoch: 313 | Loss: 0.00016891989798750728 \n",
      "Epoch: 314 | Loss: 0.00016649521421641111 \n",
      "Epoch: 315 | Loss: 0.00016410520765930414 \n",
      "Epoch: 316 | Loss: 0.0001617420930415392 \n",
      "Epoch: 317 | Loss: 0.00015941665333230048 \n",
      "Epoch: 318 | Loss: 0.00015712628373876214 \n",
      "Epoch: 319 | Loss: 0.00015487057680729777 \n",
      "Epoch: 320 | Loss: 0.0001526441192254424 \n",
      "Epoch: 321 | Loss: 0.00015044589235913008 \n",
      "Epoch: 322 | Loss: 0.00014829050633125007 \n",
      "Epoch: 323 | Loss: 0.00014615491090808064 \n",
      "Epoch: 324 | Loss: 0.0001440571213606745 \n",
      "Epoch: 325 | Loss: 0.00014198420103639364 \n",
      "Epoch: 326 | Loss: 0.00013994626351632178 \n",
      "Epoch: 327 | Loss: 0.00013793328253086656 \n",
      "Epoch: 328 | Loss: 0.00013594912888947874 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Loss: 0.00013399684394244105 \n",
      "Epoch: 330 | Loss: 0.00013207204756326973 \n",
      "Epoch: 331 | Loss: 0.0001301764277741313 \n",
      "Epoch: 332 | Loss: 0.0001282984740100801 \n",
      "Epoch: 333 | Loss: 0.00012645572132896632 \n",
      "Epoch: 334 | Loss: 0.00012464319297578186 \n",
      "Epoch: 335 | Loss: 0.0001228514884132892 \n",
      "Epoch: 336 | Loss: 0.00012108490045648068 \n",
      "Epoch: 337 | Loss: 0.00011934062786167488 \n",
      "Epoch: 338 | Loss: 0.00011762919166358188 \n",
      "Epoch: 339 | Loss: 0.00011593889212235808 \n",
      "Epoch: 340 | Loss: 0.00011426951823523268 \n",
      "Epoch: 341 | Loss: 0.00011263261694693938 \n",
      "Epoch: 342 | Loss: 0.00011101120617240667 \n",
      "Epoch: 343 | Loss: 0.0001094131002901122 \n",
      "Epoch: 344 | Loss: 0.00010784167534438893 \n",
      "Epoch: 345 | Loss: 0.0001062954033841379 \n",
      "Epoch: 346 | Loss: 0.00010476447641849518 \n",
      "Epoch: 347 | Loss: 0.00010325881885364652 \n",
      "Epoch: 348 | Loss: 0.00010177520744036883 \n",
      "Epoch: 349 | Loss: 0.00010031164856627584 \n",
      "Epoch: 350 | Loss: 9.88719766610302e-05 \n",
      "Epoch: 351 | Loss: 9.745192073751241e-05 \n",
      "Epoch: 352 | Loss: 9.604952356312424e-05 \n",
      "Epoch: 353 | Loss: 9.467201743973419e-05 \n",
      "Epoch: 354 | Loss: 9.330725879408419e-05 \n",
      "Epoch: 355 | Loss: 9.196856990456581e-05 \n",
      "Epoch: 356 | Loss: 9.064344340004027e-05 \n",
      "Epoch: 357 | Loss: 8.934605284594e-05 \n",
      "Epoch: 358 | Loss: 8.80607622093521e-05 \n",
      "Epoch: 359 | Loss: 8.67977796588093e-05 \n",
      "Epoch: 360 | Loss: 8.554713713238016e-05 \n",
      "Epoch: 361 | Loss: 8.43173183966428e-05 \n",
      "Epoch: 362 | Loss: 8.310542034450918e-05 \n",
      "Epoch: 363 | Loss: 8.191387314582244e-05 \n",
      "Epoch: 364 | Loss: 8.073040226008743e-05 \n",
      "Epoch: 365 | Loss: 7.957783964229748e-05 \n",
      "Epoch: 366 | Loss: 7.843151979614049e-05 \n",
      "Epoch: 367 | Loss: 7.730372453806922e-05 \n",
      "Epoch: 368 | Loss: 7.619273674208671e-05 \n",
      "Epoch: 369 | Loss: 7.50973413232714e-05 \n",
      "Epoch: 370 | Loss: 7.402038318105042e-05 \n",
      "Epoch: 371 | Loss: 7.29546882212162e-05 \n",
      "Epoch: 372 | Loss: 7.190410542534664e-05 \n",
      "Epoch: 373 | Loss: 7.08733859937638e-05 \n",
      "Epoch: 374 | Loss: 6.985204527154565e-05 \n",
      "Epoch: 375 | Loss: 6.885018228786066e-05 \n",
      "Epoch: 376 | Loss: 6.786226003896445e-05 \n",
      "Epoch: 377 | Loss: 6.688814028166234e-05 \n",
      "Epoch: 378 | Loss: 6.592247518710792e-05 \n",
      "Epoch: 379 | Loss: 6.497601862065494e-05 \n",
      "Epoch: 380 | Loss: 6.404152372851968e-05 \n",
      "Epoch: 381 | Loss: 6.312349432846531e-05 \n",
      "Epoch: 382 | Loss: 6.221897638170049e-05 \n",
      "Epoch: 383 | Loss: 6.13241718383506e-05 \n",
      "Epoch: 384 | Loss: 6.043856046744622e-05 \n",
      "Epoch: 385 | Loss: 5.9573296312009916e-05 \n",
      "Epoch: 386 | Loss: 5.871472603757866e-05 \n",
      "Epoch: 387 | Loss: 5.7872559409588575e-05 \n",
      "Epoch: 388 | Loss: 5.703954229829833e-05 \n",
      "Epoch: 389 | Loss: 5.622216121992096e-05 \n",
      "Epoch: 390 | Loss: 5.541327846003696e-05 \n",
      "Epoch: 391 | Loss: 5.461369073600508e-05 \n",
      "Epoch: 392 | Loss: 5.38301574124489e-05 \n",
      "Epoch: 393 | Loss: 5.3059055062476546e-05 \n",
      "Epoch: 394 | Loss: 5.2296043577371165e-05 \n",
      "Epoch: 395 | Loss: 5.1544400776037946e-05 \n",
      "Epoch: 396 | Loss: 5.080482515040785e-05 \n",
      "Epoch: 397 | Loss: 5.0073067541234195e-05 \n",
      "Epoch: 398 | Loss: 4.935274409945123e-05 \n",
      "Epoch: 399 | Loss: 4.864453512709588e-05 \n",
      "Epoch: 400 | Loss: 4.794748383574188e-05 \n",
      "Epoch: 401 | Loss: 4.7257053665816784e-05 \n",
      "Epoch: 402 | Loss: 4.65756056655664e-05 \n",
      "Epoch: 403 | Loss: 4.590581011143513e-05 \n",
      "Epoch: 404 | Loss: 4.524867836153135e-05 \n",
      "Epoch: 405 | Loss: 4.459861520444974e-05 \n",
      "Epoch: 406 | Loss: 4.395749783725478e-05 \n",
      "Epoch: 407 | Loss: 4.3325231672497466e-05 \n",
      "Epoch: 408 | Loss: 4.270134377293289e-05 \n",
      "Epoch: 409 | Loss: 4.2087642214028165e-05 \n",
      "Epoch: 410 | Loss: 4.1483996028546244e-05 \n",
      "Epoch: 411 | Loss: 4.088694186066277e-05 \n",
      "Epoch: 412 | Loss: 4.0299753891304135e-05 \n",
      "Epoch: 413 | Loss: 3.972047852585092e-05 \n",
      "Epoch: 414 | Loss: 3.9146849303506315e-05 \n",
      "Epoch: 415 | Loss: 3.8586422306252643e-05 \n",
      "Epoch: 416 | Loss: 3.803290019277483e-05 \n",
      "Epoch: 417 | Loss: 3.74855226255022e-05 \n",
      "Epoch: 418 | Loss: 3.694740735227242e-05 \n",
      "Epoch: 419 | Loss: 3.641493458417244e-05 \n",
      "Epoch: 420 | Loss: 3.58929464709945e-05 \n",
      "Epoch: 421 | Loss: 3.5376458981772885e-05 \n",
      "Epoch: 422 | Loss: 3.486714922473766e-05 \n",
      "Epoch: 423 | Loss: 3.436800398048945e-05 \n",
      "Epoch: 424 | Loss: 3.387584365555085e-05 \n",
      "Epoch: 425 | Loss: 3.338454553158954e-05 \n",
      "Epoch: 426 | Loss: 3.2906842534430325e-05 \n",
      "Epoch: 427 | Loss: 3.243225364712998e-05 \n",
      "Epoch: 428 | Loss: 3.1968666007742286e-05 \n",
      "Epoch: 429 | Loss: 3.150646443828009e-05 \n",
      "Epoch: 430 | Loss: 3.105409996351227e-05 \n",
      "Epoch: 431 | Loss: 3.0610801331931725e-05 \n",
      "Epoch: 432 | Loss: 3.01678210234968e-05 \n",
      "Epoch: 433 | Loss: 2.9734403142356314e-05 \n",
      "Epoch: 434 | Loss: 2.930726986960508e-05 \n",
      "Epoch: 435 | Loss: 2.88876035483554e-05 \n",
      "Epoch: 436 | Loss: 2.8469416065490805e-05 \n",
      "Epoch: 437 | Loss: 2.8061967896064743e-05 \n",
      "Epoch: 438 | Loss: 2.7658992621582e-05 \n",
      "Epoch: 439 | Loss: 2.726257298490964e-05 \n",
      "Epoch: 440 | Loss: 2.6869318389799446e-05 \n",
      "Epoch: 441 | Loss: 2.6483103283680975e-05 \n",
      "Epoch: 442 | Loss: 2.6102068659383804e-05 \n",
      "Epoch: 443 | Loss: 2.5728508262545802e-05 \n",
      "Epoch: 444 | Loss: 2.5357352569699287e-05 \n",
      "Epoch: 445 | Loss: 2.4995573767228052e-05 \n",
      "Epoch: 446 | Loss: 2.4633800421725027e-05 \n",
      "Epoch: 447 | Loss: 2.4279244826175272e-05 \n",
      "Epoch: 448 | Loss: 2.3931243049446493e-05 \n",
      "Epoch: 449 | Loss: 2.3586320821777917e-05 \n",
      "Epoch: 450 | Loss: 2.324838533240836e-05 \n",
      "Epoch: 451 | Loss: 2.2912336135050282e-05 \n",
      "Epoch: 452 | Loss: 2.258452877867967e-05 \n",
      "Epoch: 453 | Loss: 2.2259362594923005e-05 \n",
      "Epoch: 454 | Loss: 2.1940908482065424e-05 \n",
      "Epoch: 455 | Loss: 2.1624480723403394e-05 \n",
      "Epoch: 456 | Loss: 2.1314641344361007e-05 \n",
      "Epoch: 457 | Loss: 2.1008641851949506e-05 \n",
      "Epoch: 458 | Loss: 2.070670780085493e-05 \n",
      "Epoch: 459 | Loss: 2.0407745978445746e-05 \n",
      "Epoch: 460 | Loss: 2.0116171072004363e-05 \n",
      "Epoch: 461 | Loss: 1.982695539481938e-05 \n",
      "Epoch: 462 | Loss: 1.954317485797219e-05 \n",
      "Epoch: 463 | Loss: 1.9258637621533126e-05 \n",
      "Epoch: 464 | Loss: 1.898453956528101e-05 \n",
      "Epoch: 465 | Loss: 1.8709139112615958e-05 \n",
      "Epoch: 466 | Loss: 1.844149301177822e-05 \n",
      "Epoch: 467 | Loss: 1.817552765714936e-05 \n",
      "Epoch: 468 | Loss: 1.791518479876686e-05 \n",
      "Epoch: 469 | Loss: 1.7659163859207183e-05 \n",
      "Epoch: 470 | Loss: 1.740425796015188e-05 \n",
      "Epoch: 471 | Loss: 1.7156500689452514e-05 \n",
      "Epoch: 472 | Loss: 1.690932913334109e-05 \n",
      "Epoch: 473 | Loss: 1.6665611838106997e-05 \n",
      "Epoch: 474 | Loss: 1.6424837667727843e-05 \n",
      "Epoch: 475 | Loss: 1.6191195754799992e-05 \n",
      "Epoch: 476 | Loss: 1.5956447896314785e-05 \n",
      "Epoch: 477 | Loss: 1.5726638594060205e-05 \n",
      "Epoch: 478 | Loss: 1.550170054542832e-05 \n",
      "Epoch: 479 | Loss: 1.5278381397365592e-05 \n",
      "Epoch: 480 | Loss: 1.5059842553455383e-05 \n",
      "Epoch: 481 | Loss: 1.4841310076008085e-05 \n",
      "Epoch: 482 | Loss: 1.4629934412369039e-05 \n",
      "Epoch: 483 | Loss: 1.4419410945265554e-05 \n",
      "Epoch: 484 | Loss: 1.4211948837328237e-05 \n",
      "Epoch: 485 | Loss: 1.4008383004693314e-05 \n",
      "Epoch: 486 | Loss: 1.3807583854941186e-05 \n",
      "Epoch: 487 | Loss: 1.3607589608000126e-05 \n",
      "Epoch: 488 | Loss: 1.3412887710728683e-05 \n",
      "Epoch: 489 | Loss: 1.3220014807302505e-05 \n",
      "Epoch: 490 | Loss: 1.3029160982114263e-05 \n",
      "Epoch: 491 | Loss: 1.2841574971389491e-05 \n",
      "Epoch: 492 | Loss: 1.2658655577979516e-05 \n",
      "Epoch: 493 | Loss: 1.2476641131797805e-05 \n",
      "Epoch: 494 | Loss: 1.2296147360757459e-05 \n",
      "Epoch: 495 | Loss: 1.2120411156502087e-05 \n",
      "Epoch: 496 | Loss: 1.1946941413043533e-05 \n",
      "Epoch: 497 | Loss: 1.1773127880587708e-05 \n",
      "Epoch: 498 | Loss: 1.160553983936552e-05 \n",
      "Epoch: 499 | Loss: 1.1438757610449102e-05 \n",
      "Prediction (after training) 4 7.99611234664917\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
